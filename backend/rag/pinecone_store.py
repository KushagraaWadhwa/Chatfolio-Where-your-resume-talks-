"""
Pinecone Vector Store Implementation
Replaces ChromaDB with cloud-native Pinecone
"""

import os
from typing import List, Dict
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain.docstore.document import Document
from .data_loading import load_all_json_files
from .text_chunking import extract_section_texts, split_documents

load_dotenv()

# Pinecone configuration
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = "chatfolio"
PINECONE_HOST = "https://chatfolio-5wg1pnt.svc.aped-4627-b74a.pinecone.io"
PINECONE_EMBEDDING_MODEL = "llama-text-embed-v2"

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)


def get_pinecone_index():
    """Get or create Pinecone index"""
    try:
        # Check if index exists
        existing_indexes = pc.list_indexes()
        index_names = [idx['name'] for idx in existing_indexes]
        
        if PINECONE_INDEX_NAME not in index_names:
            print(f"Creating new Pinecone index: {PINECONE_INDEX_NAME}")
            pc.create_index(
                name=PINECONE_INDEX_NAME,
                dimension=1024,  # llama-text-embed-v2 dimension
                metric='cosine',
                spec=ServerlessSpec(
                    cloud='aws',
                    region='us-east-1'
                )
            )
            print(f"‚úÖ Index created successfully")
        
        # Connect to index
        index = pc.Index(PINECONE_INDEX_NAME, host=PINECONE_HOST)
        return index
    
    except Exception as e:
        print(f"‚ùå Error with Pinecone index: {str(e)}")
        raise


def create_pinecone_embeddings(json_directory: str, chunk_size: int = 512, overlap: int = 120):
    """
    Create embeddings and store in Pinecone
    
    Args:
        json_directory: Directory containing JSON files
        chunk_size: Size of text chunks
        overlap: Overlap between chunks
    """
    print(f"\nüîç Loading data from: {json_directory}")
    
    # 1. Load JSON files
    json_objects = load_all_json_files(json_directory)
    if not json_objects:
        print("‚ùå No JSON files found")
        return None
    
    # 2. Extract sections into documents
    docs = []
    for json_object in json_objects:
        docs.extend(extract_section_texts(json_object))
    
    # 3. Split into chunks
    split_docs = split_documents(docs, chunk_size=chunk_size, overlap=overlap)
    print(f"üìù Created {len(split_docs)} chunks from documents")
    
    # 4. Get Pinecone index
    index = get_pinecone_index()
    
    # 5. Generate embeddings using Pinecone's API and upsert
    print(f"üß† Generating embeddings with Pinecone's {PINECONE_EMBEDDING_MODEL} and uploading...")
    
    vectors_to_upsert = []
    batch_size = 96  # Pinecone API batch limit
    
    for i, doc in enumerate(split_docs):
        # Prepare text for embedding (truncate if needed)
        text = doc.page_content[:2048]  # llama-text-embed-v2 max tokens
        
        # Create vector with metadata (embedding will be generated by Pinecone)
        vector = {
            'id': f'doc_{i}',
            'metadata': {
                'text': text,
                'section': doc.metadata.get('section', 'unknown'),
                'chunk_index': i
            }
        }
        
        vectors_to_upsert.append((vector, text))
        
        # Batch upsert with embeddings
        if len(vectors_to_upsert) >= batch_size:
            # Generate embeddings via Pinecone API
            texts_batch = [v[1] for v in vectors_to_upsert]
            embeddings_response = pc.inference.embed(
                model=PINECONE_EMBEDDING_MODEL,
                inputs=texts_batch,
                parameters={"input_type": "passage"}
            )
            
            # Create vectors with embeddings
            vectors_with_embeddings = []
            for j, (vec, _) in enumerate(vectors_to_upsert):
                vec['values'] = embeddings_response[j]['values']
                vectors_with_embeddings.append(vec)
            
            # Upsert to index
            index.upsert(vectors=vectors_with_embeddings)
            print(f"   ‚úÖ Uploaded {len(vectors_with_embeddings)} vectors (total: {i+1}/{len(split_docs)})")
            vectors_to_upsert = []
    
    # Upload remaining vectors
    if vectors_to_upsert:
        texts_batch = [v[1] for v in vectors_to_upsert]
        embeddings_response = pc.inference.embed(
            model=PINECONE_EMBEDDING_MODEL,
            inputs=texts_batch,
            parameters={"input_type": "passage"}
        )
        
        vectors_with_embeddings = []
        for j, (vec, _) in enumerate(vectors_to_upsert):
            vec['values'] = embeddings_response[j]['values']
            vectors_with_embeddings.append(vec)
        
        index.upsert(vectors=vectors_with_embeddings)
        print(f"   ‚úÖ Uploaded {len(vectors_with_embeddings)} vectors")
    
    print(f"\n‚úÖ Successfully created and uploaded {len(split_docs)} embeddings to Pinecone!")
    
    # Verify
    stats = index.describe_index_stats()
    print(f"üìä Index stats: {stats['total_vector_count']} vectors in index")
    
    return index


def retrieve_from_pinecone(query: str, top_k: int = 5) -> List[Document]:
    """
    Retrieve relevant documents from Pinecone using Pinecone's embedding API
    
    Args:
        query: Search query
        top_k: Number of results to return
    
    Returns:
        List of Document objects
    """
    # Get index
    index = get_pinecone_index()
    
    # Generate query embedding using Pinecone's API
    query_embedding_response = pc.inference.embed(
        model=PINECONE_EMBEDDING_MODEL,
        inputs=[query],
        parameters={"input_type": "query"}
    )
    query_embedding = query_embedding_response[0]['values']
    
    # Search Pinecone
    results = index.query(
        vector=query_embedding,
        top_k=top_k,
        include_metadata=True
    )
    
    # Convert to LangChain Document format
    documents = []
    for match in results['matches']:
        doc = Document(
            page_content=match['metadata']['text'],
            metadata={
                'section': match['metadata']['section'],
                'score': match['score']
            }
        )
        documents.append(doc)
    
    print(f"üîç Retrieved {len(documents)} documents from Pinecone")
    for i, doc in enumerate(documents, 1):
        print(f"   {i}. Section: {doc.metadata['section']} (Score: {doc.metadata['score']:.3f})")
    
    return documents


def clear_pinecone_index():
    """Delete all vectors from Pinecone index"""
    try:
        index = get_pinecone_index()
        index.delete(delete_all=True)
        print("‚úÖ Pinecone index cleared")
    except Exception as e:
        print(f"‚ùå Error clearing index: {str(e)}")


if __name__ == "__main__":
    # Test the Pinecone setup
    print("=" * 60)
    print("üå≤ Pinecone Vector Store Setup")
    print("=" * 60)
    
    # Create embeddings
    json_directory = "backend/data"
    create_pinecone_embeddings(json_directory, chunk_size=512, overlap=120)
    
    # Test retrieval
    print("\n" + "=" * 60)
    print("üß™ Testing Retrieval")
    print("=" * 60)
    
    test_query = "What technologies did Kushagra use in SalesAssist AI?"
    docs = retrieve_from_pinecone(test_query, top_k=3)
    
    print(f"\nQuery: {test_query}")
    print(f"Results: {len(docs)} documents")

